# Default values for odoo-doodba.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

# rollDeploy activates a new deployment with each helm upgrade regardless of changes hapening or not
rollDeploy: false

# annotations for the odoo pod
annotations: {}

# updateStrategy sets the deployment.spec.strategy, default RollingUpdate:
# https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
updateStrategy:
    type: RollingUpdate

image:
  repository: tecnativa/doodba
  tag: "13.0"
  pullPolicy: IfNotPresent

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# odooPassword set the odoo database manager password a.k.a master password (random string if left empty)
# odooPassword:


# Values in config will be passed as environment variables to the odoo container
# All values starting with 'ODOO_CFG_' will be added to the odoo.conf file e.g ODOO_CFG_WORKERS = 2
config:
  PROXY_MODE: "1"
  # ODOO_CFG_SERVER_WIDE_MODULES: "session_redis"

# pgBouncer is a subchart that launches the pgBouncer connection pooler as a middleware regardless of the
# postgresql db deployed (in-cluster, external standard, gcloud )
pgBouncer:
  enabled: False

postgresql:

  image:
    repository: postgres
    tag: latest
  # Enable in-cluster deployment of the postgres instance
  enabled: true

  ## Non-root Username for Odoo database
  postgresqlUsername: "odoo"
  
  ## Database password
  # postgresqlPassword: "odoo-password"

  ### External database ###
  # All of the following values are only used when postgresql.enabled above is set to false

  #############################
  # Standard remote postgresql

  # externalDatabase: standard
  
  # Database host
  # postgresHost: postgresIP/Domain

  # Database port number
  # port: 5432

  ##############################
  # Google cloud SQL

  # externalDatabase: gcloud
  # gcInstanceConnectionName: "project-id:us-west1:instance-name"

  # Filename of svc account credentials stored in cloudsql-instance-credentials secret
  # gcCredentialsFile: "credentials.json"

  ##############################

  # Amazon AWS
  # Currently not supported

# Redis sub-chart for session storage (and potentially data caching)
redis:

  # This depends on the installation of https://github.com/camptocamp/odoo-cloud-platform/tree/13.0/session_redis
  # and adding this module to the the server_wide_modules in the odoo config file
  # config: ODOO_CFG_SERVER_WIDE_MODULES: "session_redis")

  enabled: false

  password: redis-password

  # Disable master/slave redis topology by default if redis is enabled
  cluster:
    enabled: false

service:
  type: ClusterIP
  port: 80
  longPollingPort: 8072
  # loadBalancerIP:

ingress:
  enabled: false
  # recommendedNginxSettings are a set of ingress annotations that increase nginx timeout and provides g-zip
  # compression and other opinionated proxy settings to make Odoo run smoothly.
  recommendedNginxSettings: true

  # Enable redirect from www domain to non-www domain using an ingress annotation
  wwwRedirect: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      # paths: []

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

  # Enable cache by creating a separate ingress with rules and headers just for cacheable paths (/static/*)
  cache:
    enabled: false

    #  NGINX-INGRESS config needed for this cache ingress to work (has to be setup in nginx-ingress chart)
    #  More info about this snippet on https://www.nginx.com/blog/nginx-caching-guide/
          # config:
          #   http-snippet: |
          #     proxy_cache_path /tmp/nginx-cache levels=1:2 keys_zone=static-cache:10m max_size=1g inactive=7d use_temp_path=off;
          #     proxy_cache_key $scheme$proxy_host$request_uri;
          #     proxy_cache_lock on;
          #     proxy_cache_use_stale updating;

    # Add recommended (default) nginx annotations for nginx cache to work
    recommendedNginxSettings: true

    annotations: {}

    # Hosts that will have the cache paths applied to
    # hosts:
    #   - foo.com
    #   - bar.com

    # Paths that will be added to the cache ingress (examples below make use of nginx.ingress.kubernetes.io/use-regex: true)
    # paths:
    #   - "/*/static/*"
    #   - "/web/image/*"
    #   - "/web/content/*"
    #   - "/website/image/*"

    # Same tls configuration as in the main ingress if cache is applied over SSL connection
    tls: []

persistence:
  enabled: true

  # initVolume refers to the init-container which casts chown on the odoo-volume (filestore)
  # to be set with the UID of the odoo user in the container
  initVolume: True

  ## odoo data Persistent Volume Storage Class (for filestore , sessions and apps -> /var/lib/odoo)
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  # storageClass: "-"

  # volumeName specifies an existing volume for the PVC instead of provisioning one by default
  # volumeName: odoo

  # subPath refers to a path within the mounted volume (useful for NFS shared storage and multiple environments/deployments)
  # subPath: staging

  accessMode: ReadWriteOnce
  annotations: {}
  size: 8Gi

  # persistence.existingClaim refers to the full name of an existing PVC so as to not create a new one but reuse the existing resource
  # existingClaim: 

  # persistence.extraVolumeMounts additional server volumeMounts (usefull in NFS scenarios to access live filestore for staging replication)
  extraVolumeMounts: []
  # - name: odoo
  #   mountPath: /var/lib/odoo-live
  #   subpath: live
  #   readOnly: true


# resources: 
#   requests:
#     memory: 512Mi
#     cpu: 200m
  # limits:
  #   cpu: 1
  #   memory: 2048Mi

## Configure extra options for liveness and readiness probes
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)
livenessProbe:
  enabled: true
  initialDelaySeconds: 60
  periodSeconds: 30
  timeoutSeconds: 5
  failureThreshold: 10
  successThreshold: 1

readinessProbe:
  enabled: true
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 10
  successThreshold: 1


nodeSelector: {}

tolerations: []

affinity: {}
